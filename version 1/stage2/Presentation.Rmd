---
title: "Predicting Short-Term Availability in Sydney Airbnb Listings"
author: ""
date: "date (updated: `r Sys.Date()`)"
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      afterInit: ["https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"]
---
### Project Topic
- Predict availability patterns of Sydney Airbnb listings.

<center><img src="image.jpg" width="200" alt="Airbnb Logo"></center>

- Inside Airbnb Sydney listings, June 2025.

- Classify which upcoming month shows highest availability.
---
### Research Question & Why it matters?
- Can listing features predict monthly availability patterns across Sydney?
- 
<section>
  <p>üè†  <b>Behavior Insight:</b> Quantifies how host behavior affects availability cycles.<br></p>

  <p>üß≠ <b>Supply Planning:</b> Predicts when and where supply gaps emerge.<br></p>

  <p>üß≥ <b>Smart Travel:</b> Helps travelers find better time to book stays.<br></p>
</section>

- Listing features can predict availability patterns.
---
### Data Description
```{r, message=FALSE, echo=FALSE}
library(readr)
library(tidyverse)
df <- read_csv("../datasets/listings.csv")
observations = dim(df)[1]
variables = dim(df)[2]
numeric = sum(sapply(df, is.numeric))
logical = sum(sapply(df, is.logical))
categorical = sum(sapply(df, is.character))
cate_total = categorical + logical
uniques <- sapply(df, function(x) n_distinct(x, na.rm = TRUE))
overall_missing_pct <- round((sum(is.na(df))/(nrow(df)*ncol(df)))*100,2)
miss_by_var <- data.frame(
  variable = names(df),
  n_missing = colSums(is.na(df))) %>%
  mutate(
    pct_missing = round(100 * n_missing / nrow(df), 2)) %>%
  arrange(desc(pct_missing))
total_missing_cells <- sum(miss_by_var$n_missing)
miss_conc <- miss_by_var %>%
  mutate(prop = n_missing / total_missing_cells, cum_prop = cumsum(prop))
k_vals <- c(3, 5, 10)
topk_cover <- lapply(k_vals, function(k){
  tibble(
    k = k,
    topk_cum_pct_of_all_missing = round(100 * miss_conc$cum_prop[k], 2),
    topk_vars = paste(miss_conc$variable[seq_len(min(k, nrow(miss_conc)))], collapse = ", "))}) %>% bind_rows()
thresh <- c('>50' = 50, '>20' = 20, '>10' = 10, '>5' = 5, '>1' = 1)
thresh_tbl <- tibble(
  threshold_pct = names(thresh),
  n_vars = sapply(unname(thresh), function(t) sum(miss_by_var$pct_missing > t)))
n_zero_missing <- sum(miss_by_var$n_missing == 0)
```

- `r observations` listings √ó `r variables` variables

- Mix of `r numeric` numeric and `r cate_total` categorical features

- `r sum(uniques > 20)` variables have more than 20 unique levels

- Overall missing rate: `r overall_missing_pct`%

- Fully complete variables: `r n_zero_missing`

- `r thresh_tbl$n_vars[thresh_tbl$threshold_pct=='>10']` variables have >10% missingness
---
### Data Cleaning and Preparation

<div class="columns-2" style="display: flex;align-items: flex-start;justify-content: flex-start; gap: 10px;">
<div style="flex: 0 0 55%; margin-top:-50px">
<br></br>
<p><b>Outcome Variable</b></p>
<ul>
<li> Categorical target: month with the highest availability (next 3 months)
<br></br>
<li> Captures short-term temporal variation in Airbnb availability
<br></br>
<li> Recast forward availability into 3 windows (0‚Äì30 / 31‚Äì60 / 61‚Äì90 days), ‚ÄúIndeterminate‚Äù for ties
<br></br>
<li> Forms a multi-class classification problem
</ul>
</div>
<div style="flex: 0 0 45%; text-align:left; zoom:1.5; transform-origin: top; margin-top:-150px; margin-left: 10px">

```{r, echo=FALSE}
library(DiagrammeR)
DiagrammeR::mermaid("
graph TB 
A(Feature Engineering) --> B(Sanity Filtering)
B --> C(Outcome Construction)
C --> D(Drop Irrelevant Columns)
", width = "500px", height = "580px")
```
</div>
</div>
---
### Data After Cleaning
```{r, warning=FALSE, message=FALSE, echo=FALSE}
library(knitr)
library(kableExtra)
dfcleaned <- read_csv("../datasets/cleaned_data.csv")
observationscl = dim(dfcleaned)[1]
variablescl = dim(dfcleaned)[2]
numericcl = sum(sapply(dfcleaned, is.numeric))
logicalcl = sum(sapply(dfcleaned, is.logical))
categoricalcl = sum(sapply(dfcleaned, is.character))
cate_totalcl = categorical + logical
uniquescl <- sapply(dfcleaned, function(x) n_distinct(x, na.rm = TRUE))
overall_missing_pctcl <- round((sum(is.na(dfcleaned))/(nrow(dfcleaned)*ncol(dfcleaned)))*100,2)

summary_table <- data.frame(
  Item = c("Size", "Feature types", "Number of High-cardinality Variables", "Overall Missing Rate(%)"),
  `Before Cleaning` = c(
    paste(observations, "listings √ó", variables, "variables"),
    paste(numeric, "numeric,", cate_total, "categorical"),
    paste(sum(uniques > 20)),
    paste(overall_missing_pct)
  ),
  `After Cleaning` = c(
    paste(observationscl, "listings √ó", variablescl, "variables"),
    paste(numericcl, "numeric,", cate_totalcl, "categorical"),
    paste(sum(uniquescl > 20)),
    paste(overall_missing_pctcl)
  )
)
kable(summary_table, align = c("l", "l", "l"),   
      booktabs = FALSE,
  linesep = "") %>%
  kable_paper(full_width = TRUE, html_font = "Arial") %>%
  column_spec(1, bold = TRUE, width = "5cm") %>%
  kable_styling(
    bootstrap_options = c("bordered"),
    position = "center",
    full_width = TRUE,
    font_size = 20)
```

---
### Exploary Data Analysis

```{r, echo=FALSE, message=FALSE, dev='svg', warning=FALSE, fig.width=20, fig.height=15}
library(ggplot2)
library(dplyr)
library(patchwork)
library(ggcorrplot)

palette <- c("#4C72B0", "#55A868", "#C44E52", "#8172B3")

num_df <- dfcleaned %>% select(where(is.numeric))
corr <- cor(num_df, use = "pairwise.complete.obs")
num_long <- dfcleaned |> select(where(is.numeric)) |> pivot_longer(cols = everything(), names_to = "variable", values_to = "value")
num_scaled <- num_long %>%
  group_by(variable) %>%
  mutate(scaled_value = scale(value))

p1 <- ggplot(
  dfcleaned |> mutate(outcome = factor(
    outcome,
    levels = c("First-month", "Second-month", "Third-month", "Indeterminate"))),
  aes(x = outcome, fill = outcome)) +
  geom_bar(show.legend = FALSE) +
  scale_fill_manual(values = palette) +
  labs(title = "Outcome Class Distribution") +
  theme(
    panel.grid.minor = element_blank(),
    panel.grid.major.x = element_blank(),        
    plot.background = element_blank(),   
    plot.title = element_text(size = 15, face = "bold", hjust = 0.5),
    axis.text.x = element_text(size = 10, angle = 10, vjust = 1),
    axis.title.x = element_blank(),
    axis.title.y = element_blank())


p2 <- ggcorrplot(
  corr,
  lab = FALSE,
  colors = c("#4C72B0", "white", "#C44E52")) +
  labs(title = "Correlation Heatmap") +
  theme_minimal(base_size = 10) +
  theme(
    plot.title = element_text(size = 15, face = "bold", hjust = 0.5),
    legend.position = "none", 
    axis.title.x = element_blank(),
    axis.title.y = element_blank(),
    axis.text.x = element_text(size = 10, angle = 45, hjust = 1),
    axis.text.y = element_text(size = 10))

p3 <- ggplot(num_scaled, aes(x = variable, y = scaled_value)) +
  geom_violin(fill = "#4C72B0", alpha = 0.8) +
  labs(
    title = "Normalized Distributions (Z-score)",
    x = NULL, y = "Z-score"
  ) +
  theme_minimal(base_size = 15) +
  theme(
    plot.title = element_text(size = 15, face = "bold", hjust = 0.5),
    axis.text.x = element_text(angle = 15, size = 15, hjust = 1))

(p1 + p2 + plot_layout(widths = c(1.3, 0.7))) /
  p3 +
  plot_layout(heights = c(0.8, 1.2))
```
---
### Data Preprocessing for Modeling
```{r, echo=FALSE,message=FALSE}
library(DiagrammeR)
DiagrammeR::mermaid("
graph TD
  subgraph Model Evaluation Metrics
    C1(Define metric set)--> C2(Define metrics evaluation)
    C2 --> C3(Compute Accuracy, Macro-F1,and Macro ROC-AUC)
  end

    B(Recipe Preprocessing per CV fold) --> B1(Convert characters to factors)
  subgraph Recipe Preprocessing per CV fold
    B1 --> B2(Merge rare categories smaller than 2%)
    B2 --> B3(Impute missing numeric values with median)
    B3 --> B4(Log-transform skewed variables)
    B4 --> B5(Dummy encode categorical variables)
    B5 --> B6(Normalize numeric predictors)
    B6 --> B7(Upsample target variable)
  end

  subgraph Data Split & Cross-Validatiom
    A1(Start: Raw dataset) --> A2(Initial split:80% training / 20% testing, stratified by outcome)
    A2 --> A3(Training subset 80%)
    A3 --> A5(10-fold cross-validation, stratified by outcome)
    A2 --> A4(Testing subset 20%)
  end

A5 --> B
A4 --> C3
B --> C3
B7 -->|repeat for next fold| B1
", width = "800px", height = "550px")
```
---
### What model(s) are you using? Why?
- definition, assumption, interpretability, tune
- performance metrics selection
---
### Key Results and Visuals
- result for performance metrics in table
- visual of actual vs predicted and tuning result for best model
---
### Final Insighs: What have you learned & link back to the research problem
- findings
- answer research question
- reflect result in real life
- limitation
- future direction
