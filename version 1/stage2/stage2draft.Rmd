---
title: "stage2draft"
author: "Mia"
date: "2025-10-05"
output:
  html_document:
    number_sections: true
    code_folding: hide
---

```{r setup, include=FALSE}
library(readr)
library(tidyverse)
library(janitor)
library(DataExplorer)
library(ggplot2)
library(GGally)
library(skimr)

knitr::opts_chunk$set(warning   = FALSE,message   = FALSE)
```

# Proposal Draft
```{r, message=FALSE}
df <- read_csv("../datasets/cleaned_data.csv")
```


```{r}
num_vars <- df %>%
  select(where(is.numeric))

plot_density(num_vars)

num_vars_long <- num_vars %>%
  pivot_longer(cols = everything(), names_to = "variable", values_to = "value")

ggplot(num_vars_long, aes(x = variable, y = value)) +
  geom_boxplot(outlier.color = "red", fill = "lightblue") +
  coord_flip() +
  labs(title = "Boxplots of Numeric Variables",
       x = "Variable", y = "Value") +
  theme_minimal()
```


# Model Draft
```{r}
library(tidymodels)
library(themis)
library(yardstick)
library(kableExtra)
library(doParallel)
cores <- detectCores() - 1
registerDoParallel(cores = cores)
```

```{r}
df <- df |> 
  filter(outcome != "Indeterminate") |> 
  select(-room_type, -neighborhood_overview) |> 
  mutate(outcome = factor(outcome)) |>
  select(-beds)
```


```{r}
split <- initial_split(df, prop = 0.8, strata = outcome)
train <- training(split)
test <- testing(split)
folds <- vfold_cv(train, v = 3, strata = outcome)

rec <- recipe(outcome ~ ., data = train) |>
  step_string2factor(all_nominal_predictors()) |> 
  step_other(neighbourhood_cleansed, threshold = 0.02) |> 
  step_impute_median(all_numeric_predictors()) |> 
  step_log(price, number_of_reviews, reviews_per_month, offset = 1) |> 
  step_dummy(all_nominal_predictors(), one_hot = FALSE) |> 
  step_normalize(all_numeric_predictors()) |>
  step_upsample(outcome)


metric_set_all <- metric_set(accuracy, f_meas, roc_auc)
```

```{r}
library(ranger)
library(xgboost)
set.seed(5003)
# 模型（多分类逻辑回归）
mod_logit <- multinom_reg(
  penalty = tune(),   # 调参用
  mixture = 0         # 0 = ridge 正则化 (更稳)
) |>
  set_engine("glmnet") |>
  set_mode("classification")

# 工作流
wf_logit <- workflow() |>
  add_model(mod_logit) |>
  add_recipe(rec)

grid_logit <- grid_regular(penalty(range = c(-4, 0)), levels = 10)

# 交叉验证
res_logit <- tune_grid(
  wf_logit,
  resamples = folds,
  grid = grid_logit,
  metrics = metric_set(accuracy, f_meas, roc_auc)
)

best_logit <- select_best(res_logit, metric = "f_meas")
final_logit <- finalize_workflow(wf_logit, best_logit)
fit_logit <- fit(final_logit, data = train)

# Decision Tree
mod_tree <- decision_tree(tree_depth = tune(), cost_complexity = tune()) |>
  set_engine("rpart") |> set_mode("classification")
wf_tree  <- workflow() |> add_model(mod_tree)  |> add_recipe(rec)
grid_tree <- grid_regular(tree_depth(range = c(2L, 10L)), cost_complexity(), levels = 5)
res_tree <- tune_grid(wf_tree, resamples = folds, grid = grid_tree, metrics = metric_set_all)
best_tree <- select_best(res_tree, metric = "f_meas")
final_tree <- finalize_workflow(wf_tree, best_tree)
fit_tree <- fit(final_tree, data = train)

# Random Forest
mod_rf <- rand_forest(mtry = tune(), trees = 300, min_n = tune()) |>
  set_engine("ranger", importance = "impurity") |> set_mode("classification")
wf_rf    <- workflow() |> add_model(mod_rf)    |> add_recipe(rec)
grid_rf <- grid_regular(mtry(range = c(2L, floor(sqrt(ncol(train))))),
                        min_n(range = c(2L, 10L)), levels = 5)
res_rf <- tune_grid(wf_rf, resamples = folds, grid = grid_rf, metrics = metric_set_all)
best_rf <- select_best(res_rf, metric = "f_meas")
final_rf <- finalize_workflow(wf_rf, best_rf)
fit_rf <- fit(final_rf, data = train)

# XGBoost
mod_xgb <- boost_tree(trees = 300, learn_rate = tune(), tree_depth = tune(),
                      loss_reduction = tune(), min_n = tune()) |>
  set_engine("xgboost", eval_metric = "mlogloss") |> set_mode("classification")
wf_xgb   <- workflow() |> add_model(mod_xgb)   |> add_recipe(rec)
grid_xgb <- grid_random(
  learn_rate(range = c(0.05, 0.3)),
  tree_depth(range = c(3L, 8L)),
  loss_reduction(range = c(0, 1)),
  min_n(range = c(2L, 8L)),
  size = 5
)
res_xgb <- tune_grid(wf_xgb, resamples = folds, grid = grid_xgb, metrics = metric_set_all)
best_xgb <- select_best(res_xgb, metric = "f_meas")
final_xgb <- finalize_workflow(wf_xgb, best_xgb)
fit_xgb <- fit(final_xgb, data = train)
```

```{r}
eval_metrics <- function(model_fit, test_data) {
  test_data$outcome <- factor(test_data$outcome,
                              levels = levels(train$outcome))
  
  pred <- predict(model_fit, test_data, type = "prob") |>
    bind_cols(predict(model_fit, test_data)) |>
    bind_cols(test_data |> select(outcome))
  
  acc <- accuracy(pred, truth = outcome, estimate = .pred_class) |> pull(.estimate)
  f1  <- f_meas(pred, truth = outcome, estimate = .pred_class, estimator = "macro") |> pull(.estimate)
  auc <- tryCatch({
    roc_auc(pred, truth = outcome,
            dplyr::matches("^\\.pred_(?!class)", perl = TRUE),
            estimator = "hand_till") |>
      dplyr::pull(.estimate) |>
      as.numeric()
  }, error = function(e) NA_real_)
  
  tibble(Accuracy = acc, Macro_F1 = f1, Macro_ROC_AUC = auc)
}
```

```{r}
test$outcome <- factor(test$outcome, levels = levels(train$outcome))

results <- bind_rows(
  eval_metrics(fit_logit, test) |> mutate(Model = "Logit Regression"),
  eval_metrics(fit_tree,  test) |> mutate(Model = "Decision Tree"),
  eval_metrics(fit_rf,    test) |> mutate(Model = "Random Forest"),
  eval_metrics(fit_xgb,   test) |> mutate(Model = "XGBoost")
) |> select(Model, Accuracy, Macro_F1, Macro_ROC_AUC)

kable(results,
      digits = 3, align = "c") |>
  kable_styling(full_width = FALSE, position = "center",
                bootstrap_options = c("hover", "condensed"))
```

```{r}

```